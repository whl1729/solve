# Q186 神经网络的加速原理是什么

## 版本说明

| 时间 | 版本 | 说明 |
| ---- | ---- | ---- |
| 2023-12-22 | 1.0 | 添加2篇综述论文的链接 |
| 2023-12-25 | 1.1 | 概括两篇综述论文的观点，得出初步结论 |

## 解答

目前看了两篇神经网络加速器综述，暂时得出以下结论：

神经网络的加速可以是硬件或软件（算法）层面的。

- 硬件层面，可以通过优化芯片设计，提高访存效率；
- 软件层面，可以优化指令集、优化编译方法、优化数据流等，也可以针对特定的数据进行特定的优化。

### 神经网络加速器架构概述

- 神经处理单元 (Neural Processing Unit, NPU)
  - 为加速通用程序段中可以用 NN 做近似计算 (approximate computation) 来替代的部分例如（索贝尔边缘检测 (sobel edge detection) 和快速傅立叶变换 (fast fourier transform)）而设计的专用硬件

- DianNao 系列
  - 提出了一种针对减小访存延时的特殊设计
    - 在片上集成了大量嵌入式动态随机存取存储器（embedded Dynamic Random Access Memory, eDRAM）来尽可能避免对片外主存的读取
    - 当 CNN 模型参数量小的时候，我们就可以把 CNN 的权重全都存储到片上静态随机存取存储器（SRAM）上
  - 为了同时支持这些应用的不同数据访存模式， PuDianNao 提出了“冷缓存” (cold buﬀer) 和“热缓存” (hot buﬀer) 来应对不同的数据重用方式
  - 一些如循环展开 (loop unrolling) 和循环分块（loop tiling）等编译技术也被应用，从而形成一个软硬件协同设计的方法来增加片上数据的重用以及 PE 的实际利用率。

- 张量处理单元(Tensor Processing Unit, TPU)

- 寒武纪(Cambricon)是一个以 load-store 架构为基础，集成标量，向量，矩阵，逻辑，数据传输及控制指令的一个面向 DNN 加速的 ISA。这个 ISA 设计考虑了数据并行，定制的向量/矩阵操作指令以及对 scratchpad memory 的使用等多重因素。

- 数据流分析
  - Eyeriss 针对 NN 计算的特点提出了三种不同的数据流，分别是“输入固定”(Input-Stationary, IS)，“输出固定”(Output-Stationary, OS) 以及“权重固定” (Weight-Stationary, WS)。
  - 在这三种数据流的基础上，结合具体的加速器硬件设计， Eyeriss 进一步提出了“行”固定(Row-Stationary, RS)的数据流来进一步提高数据的重用。
  - 工业界的许多加速器产品也都重视数据流的设计与实现。
    - WaveComputing 为可重构阵列加速器提出了粗粒度调配的数据流；
    - GraphCore 则专门为图计算设计高效的架构及数据流等等。

- 软硬件协同设计及面向新型应用的加速器
  - DNN 加速器的性能也可以通过高效且适用于硬件计算特点的 NN 算法来提升。
    - 例如通过对 NN 的剪枝来减少模型的参数量并且使其更加稀疏，以此来减少数据访存。
    - 而对 NN 中参数的量化则可以使 NN 计算更加低精度，既减少数据的存储量又能降低计算成本。
  - 一些新型的应用比如对抗神经网络 (Generative Adversarial Network, GAN)，基于深度学习的推荐系统等，为专用加速器等设计提出了新的要求

- 稀疏神经网络加速
  - Cambricon-X 和 Cambricon-S 就采用了一种软硬件协同的方法去解决稀疏 NN 模型计算中不规则访存的问题。
  - ReCom 则针对 ReRAM 平台提出了一种将权重和输入（包含中间结果）结构化压缩的方案来加速稀疏 NN 的推理。
  - ReBoc 则在 ReRAM 平台上对采用“块循环” (block circulant) 压缩方法的 NN 进行加速。
  - PRA 对串行输入中每个数据中的“0”比特进行跳过操作，因而节省了不必要的计算开销。并且对此操作提出了相应的定制乘加器设计。
  - Laconic 则对 NN 计算中的乘法进行比特粒度的分解，从而最大可将必要的计算量降低 40 倍。

### [深度神经网络加速器体系结构概述][2]

- 最近的特定于AI的计算系统（即AI加速器）通常由大量高度并行的计算和存储单元构成。
  - 这些单元以二维方式组织，以支持神经网络（NN）中常见的矩阵向量乘法。
  - 片上网络（NoC）、高带宽存储器（HBM）和数据重用等被用于进一步优化这些加速器中的数据流。
  - 生物逻辑理论基础、硬件设计和算法（软件）这三个层次的创新是AI加速器的三个基石。

- 尽管DNN可能包含许多类型的层，但是矩阵乘法和卷积占了90%以上的运算，并且是DNN加速器设计的主要优化目标。

- 2012年，惠普实验室提出了一种ReRAM交叉结构（crossbar），可以有效地加速神经网络中的矩阵矢量乘法。

- 在DNN加速器设计的早期阶段，加速器被设计用于加速通用处理中的近似程序或用于小型神经网络。

- 片上加速器
  - 神经处理单元（neural processing unit, NPU）
  - RENO：一种可重新配置的 NoC 加速器

## 参考资料

- [神经网络加速器架构概述][1]
- [深度神经网络加速器体系结构概述][2]

  [1]: https://www.sciengine.com/SSI/article;JSESSIONID=48bd889f-2952-4623-8d60-bcf04561b5f8?doi=10.1360/SSI-2021-0409&scroll=
  [2]: https://www.engineering.org.cn/ch/10.1016/j.eng.2020.01.007
